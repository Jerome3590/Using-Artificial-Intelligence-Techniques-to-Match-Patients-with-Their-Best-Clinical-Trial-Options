---
title: "ORNL Deep Learning Model for Data Science Challenge"
author: "Jerome Dixon"
date: "8/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

library(reticulate)
use_condaenv("ORNL")

```


```{r}

library(here)
library(readxl)
library(httr)
library(jsonlite)
library(dplyr)
library(tidyr)
library(magrittr)
library(aws.s3)
library(tidyverse)
library(kableExtra)
library(purrr)
library(data.table)
library(tm)
library(caret)
library(keras)

```


```{r Import DL dataset}


final_DL_df <- read_csv("data/nlp_stage3/final_DL_df.csv")

final_DL_df %<>% select(2:6)


```



```{r Shrink Dataset}

final_DL_df$eigencentrality <- as.numeric(final_DL_df$eigencentrality)
final_DL_df$target_site <- as.factor(final_DL_df$target_site)


final_DL_df$pareto <- ntile(final_DL_df$eigencentrality, 40)  


sample_dl_df <- final_DL_df %>% filter(pareto == 1 & eigencentrality > 0 & eigencentrality_adjusted >0)


dl_dataset <- select(sample_dl_df, 1:5)


```



```{r Transform to Matrix}

nlp_data <- as.matrix(dl_dataset)

dimnames(nlp_data) <- NULL

nlp_data[,5] <- as.numeric(nlp_data[,5]) - 1

```


```{r Test and Training Splits}

set.seed(1997)

indx <- sample(2, nrow(nlp_data), replace = TRUE, prob = c(0.8, 0.2))


train <- nlp_data[indx==1, 1:4]
test <- nlp_data[indx==2, 1:4]

train_target <- nlp_data[indx==1, 5]
test_target  <- nlp_data[indx==2, 5]


```


```{r One Hot Encoding}

train_labels <- to_categorical(train_target)
test_labels <- to_categorical(test_target)

print(test_labels[1:10,])


```


```{r Scale Features}


mean_train <- apply(x_train, 2, mean)

std_train <- apply(x_train, 2, sd)

x_train <- scale(x_train, center = mean_train, scale = std_train)

x_test <- scale(x_test, center = mean_train, scale = std_train)


```


```{r}

# Creating the model

model <- keras_model_sequential()
model %>% 
  layer_dense(units = 4,
              kernel_regularizer = regularizer_l2(0.001),
              activation = "relu",
              input_shape = c(4)) %>% 
  layer_dropout(rate = 0.2) %>% 
  layer_dense(units = 50,
              kernel_regularizer = regularizer_l2(0.001),
              activation = "relu") %>% 
  layer_dense(units = 25,
              activation = "softmax")
summary(model)


```


```{r}

# Compiling the model
model %>% compile(loss = "categorical_crossentropy",
                  optimizer = "adam",
                  metrics = c("accuracy"))



```



```{r}


history <- model %>% 
  fit(x_train,
      y_train,
      epoch = 10,
      batch_size = 1,
      validation_split = 0.2,
      verbose= 2)


```


```{r}

plot(history)

```


```{r}

model %>% 
  evaluate(x_test,
           y_test)


```




```{r}


pred <- model %>% 
  predict_classes(x_test)

table(Predicted = pred,
      Actual = y_test_actual)



```


```{r}

prob <- model %>% 
  predict_proba(x_test)
cbind(prob,
      pred,
      y_test_actual)



```

