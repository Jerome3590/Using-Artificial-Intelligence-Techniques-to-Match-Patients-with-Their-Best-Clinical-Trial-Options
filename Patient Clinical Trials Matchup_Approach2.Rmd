---
title: "ORNL Data Challenge"
author: "Jerome Dixon"
date: "7/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}

library(reticulate)
use_condaenv("data-challenge")

```


```{r}

library(here)
library(readxl)
library(httr)
library(jsonlite)
library(dplyr)
library(magrittr)
library(aws.s3)
library(tidyverse)
library(kableExtra)
library(purrr)
library(data.table)
library(tm)
library(text2vec)
library(keras)


```


```{r AWS ORNL Data Files}

aws_bucket <- "s3://ornl-data-challenge/"

bucket_df <- get_bucket_df(aws_bucket, check_region = F)

bucket_df$Key

#Download AWS files to your local directory

```


```{r Local ORNL Data Files}
#Sample Participants
participants_marked <- read_excel(here("data","participants_data", "Dataset2_Sample_Participant_Data_With_Syn_Columns_Marked.xlsx"))

#Eligibility Criteria
files_e <- list.files(path = here("data","eligibility_criteria"), pattern = "*.xlsx", full.names = T)

eligibility_rules <- sapply(files_e, read_excel, simplify=FALSE) %>% 
bind_rows(.id = "id")

#Clinician Matches
files_c <- list.files(path = here("data","clinician_matches"), pattern = "*.xlsx", full.names = T)

clinician_matches <- sapply(files_c, read_excel, simplify=FALSE) %>% 
bind_rows(.id = "id")

```


```{r Test Files}

test_data_titles <- clinician_matches[2,c(3:51)] %>% t()
test_data_titles %<>% as.data.frame() %>% unique()
rownames(test_data_titles) <- c()
test_data_titles = test_data_titles[-1,]
test_data_titles %<>% as.list()

test_data_nct <- clinician_matches[1,c(3:51)] %>% t()
test_data_nct %<>% as.data.frame() %>% unique()
rownames(test_data_nct) <- c()
test_data_nct = test_data_nct[-1,]
test_data_nct %<>% as.list()


```



```{r Clinical Trials Data From NCI API}

base_url <- "https://clinicaltrialsapi.cancer.gov/v1/clinical-trials?include=official_title&anatomic_sites"

cancer_sites <- read_csv("data/cancer-sites.csv")

cancer_sites$URL <- paste0(base_url,'=',cancer_sites$`Cancer Sites`,sep="")

URL <- as.data.frame(cancer_sites$URL)
names(URL) <- "URL"

clinical_trials_title <- map_df(URL$URL, ~{

  Sys.sleep(sample(seq(0.5, 2.5, 0.5), 1)) 

  res <- httr::GET(.x)

  content(res, as="text", encoding="UTF-8") %>% 
    fromJSON(flatten=TRUE) %>% 
    as_tibble() 

})


```



```{r Build Datasets for Deep Learning}

clinical_trials_title %<>% unnest(cols = c(trials))


clinical_trials_title$total <- as.factor(clinical_trials_title$total)

names(clinical_trials_title) <- c("class", "title")


DL_df <- clinical_trials_title %>% select(`title`, everything())

DL_ls <- as.list(DL_df)

DL_ls_one <- unlist(DL_ls)

DL_vc <- as.vector(DL_df)

DL_mx <- as.matrix(DL_df)

write.csv(DL_df, "DL_Labeled_Data.csv")


```

```{r}

DL_dtm <- VectorSource(DL_mx) %>%
  VCorpus() %>%
  DocumentTermMatrix(control = list(removePunctuation = TRUE,
                                    removeNumbers = TRUE,
                                    stopwords = TRUE))

inspect(DL_dtm)

```

```{r}

prep_fun = tolower
tok_fun = word_tokenizer

it_train = itoken(DL_df$title, 
             preprocessor = prep_fun, 
             tokenizer = tok_fun, 
             ids = row.names(DL_df),
             progressbar = FALSE)
vocab = create_vocabulary(it_train)


```


```{r}

train_tokens = tok_fun(prep_fun(DL_df$title))
  
it_train2 = itoken(train_tokens, 
                  ids = row.names(DL_df),
                  progressbar = FALSE)

vocab2 = create_vocabulary(it_train2)
vocab2

```


```{r}

vectorizer = vocab_vectorizer(vocab2)
t1 = Sys.time()
dtm_train = create_dtm(it_train2, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))



```

```{r}

dim(dtm_train)

dim(DL_dtm)


```
```{r}

dtm_normalized_l1 = normalize(dtm_train, "l1")


```


```{r}

# Note that most text2vec functions are pipe friendly!
it_test = tok_fun(prep_fun(DL_df$class))

# turn off progressbar because it won't look nice in rmd
it_test = itoken(it_test, ids = row.names(DL_df), progressbar = FALSE)
         

dtm_test = create_dtm(it_test, vectorizer)


```







```{r}

vocab3 = create_vocabulary(it_train2)

vectorizer = vocab_vectorizer(vocab3)

dtm_train2 = create_dtm(it_train, vectorizer)

# define tfidf model
tfidf = TfIdf$new()
# fit model to train data and transform train data with fitted model

dtm_train_tfidf = fit_transform(dtm_train2, tfidf)

# tfidf modified by fit_transform() call!
# apply pre-trained tf-idf transformation to test data
dtm_test_tfidf = create_dtm(it_test, vectorizer)
dtm_test_tfidf = transform(dtm_test_tfidf, tfidf)



```




```{r}


text <- DL_df$title

max_features <- 1000
tokenizer <- text_tokenizer(num_words = max_features)

tokenizer %>% 
  fit_text_tokenizer(text)

sequences = texts_to_sequences(tokenizer,text)

DL_df$title <- sequences


```



```{r}

labels = DL_df$class

labels %<>% toString()

labels_encoded <- text_one_hot(labels, 50, lower=TRUE)

labels_df <- as.data.frame(labels_encoded)

```

```{r}

data_df <- as.data.frame(data)


```


```{r}

dl_df_nlp <- merge(data_df,labels_df)

set.seed(1997)

indexes <- createDataPartition(DL_df$class, times = 1,
                               p = .8, list = FALSE)

train <- DL_df[indexes,]
test <- DL_df[-indexes,]

list1 <- as.list(x = train$title) 

list2 <- as.list(x= test$title) 


list5 <- list(x = list1, y = train$class)
list6 <- list(y = list2, y = test$class)

titles <- list(train = list5, test = list6)



```



```{r}

model <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = 46, activation = "softmax")



```

```{r}

model %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)


```


```{r}

history <- model %>% fit(
  x_train,
  training_labels,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_test, testing_labels)
)


```



```{r}

#Initialize model
model <- keras_model_sequential()

model %>% 
  # Start off with an efficient embedding layer which maps
  # the vocab indices into embedding_dims dimensions
  layer_embedding(max_features, embedding_dims, input_length = maxlen) %>%
  layer_dropout(0.2) %>%

  # Add a Convolution1D, which will learn filters
    # Word group filters of size filter_length:
  layer_conv_1d(
    filters, kernel_size, 
    padding = "valid", activation = "relu", strides = 1
  ) %>%
  # Apply max pooling:
  layer_global_max_pooling_1d() %>%

  # Add a vanilla hidden layer:
  layer_dense(hidden_dims) %>%

  # Apply 20% layer dropout
  layer_dropout(0.2) %>%
  layer_activation("relu") %>%

  # Project onto a single unit output layer, and squash it with a sigmoid

  layer_dense(1) %>%
  layer_activation("sigmoid")

# Compile model
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)



```


```{r}

hist <- model %>%
  keras::fit(
    x_train,
    one_hot_train_labels,
    batch_size = batch_size,
    epochs = epochs,
    validation_split = 0.3
  )



plot(hist)

```










































```{r}

library(keras)
library(tokenizer)
library(stopwords)

tokenizer <- text_tokenizer(num_words = 500,lower=TRUE) %>%                         
  fit_text_tokenizer(DL_ls_one)                                        

sequences <- texts_to_sequences(tokenizer, DL_ls_one)                  

one_hot_results <- texts_to_matrix(tokenizer, DL_ls_one, mode = "binary")   

word_index <- tokenizer$word_index                                        
cat("Found", length(word_index), "unique tokens.\n")


```
```{r}

library(tokenizers)
library(stopwords)
library(data.table)
library(text2vec)

titles_train <- tokenize_sentences(DL_ls$title)

labels_train <- DL_ls$class




```




