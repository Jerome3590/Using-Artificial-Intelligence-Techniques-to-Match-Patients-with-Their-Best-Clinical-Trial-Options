---
title: "ORNL Data Challenge"
author: "Jerome Dixon"
date: "7/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}

library(reticulate)
use_condaenv("ORNL")

```


```{r}

library(here)
library(readxl)
library(httr)
library(jsonlite)
library(dplyr)
library(magrittr)
library(aws.s3)
library(tidyverse)
library(kableExtra)
library(purrr)
library(data.table)


```


```{r AWS ORNL Data Files}

aws_bucket <- "s3://ornl-data-challenge/"

bucket_df <- get_bucket_df(aws_bucket, check_region = F)

bucket_df$Key

#Download AWS files to your local directory

```


```{r Local ORNL Data Files}

#Sample Participants
participants_marked <- read_excel(here("data","participants_data", "Dataset2_Sample_Participant_Data_With_Syn_Columns_Marked.xlsx"))

#Eligibility Criteria
files_e <- list.files(path = here("data","eligibility_criteria"), pattern = "*.xlsx", full.names = T)

eligibility_rules <- sapply(files_e, read_excel, simplify=FALSE) %>% 
bind_rows(.id = "id")

#Clinician Matches
files_c <- list.files(path = here("data","clinician_matches"), pattern = "*.xlsx", full.names = T)

clinician_matches <- sapply(files_c, read_excel, simplify=FALSE) %>% 
bind_rows(.id = "id")


```



```{r Clinical Trial Data From NCI API}

base_url <- "https://clinicaltrialsapi.cancer.gov/v1/clinical-trials?anatomic_sites"

cancer_sites <- read_csv("data/cancer-sites.csv")

cancer_sites$URL <- paste0(base_url,'=',cancer_sites$`Cancer Sites`,sep="")

URL <- as.data.frame(cancer_sites$URL)
names(URL) <- "URL"

clinical_trials_list <- map(URL$URL, ~{

  Sys.sleep(sample(seq(0.5, 2.5, 0.5), 1)) 

  res <- httr::GET(.x)

  content(res, as="text", encoding="UTF-8") %>% 
    fromJSON(flatten=TRUE) %>% 
    as_tibble() 

})


```


```{r NLP Files for Processing }

#Create 'nlp' folder to store files under existing 'data' folder

names(clinical_trials_list) <- c(1:25)

for(i in seq_along(clinical_trials_list)) {
   dom <- clinical_trials_list[[i]][["trials"]] %>% 
      select(c(1:2,18:19,22:23,39))

    dom_nlp <- as.data.frame(dom) %>% 
      unnest(c(7))

    write.csv(dom_nlp, paste0("data/nlp_in/", dom_nlp$anatomic_sites[i], names(clinical_trials_list)[i],".csv"))
}


```



```{python NLP Processor Stage I}

import pytextrank
import spacy
from math import sqrt
import pandas as pd


nlp = spacy.load("en_core_web_sm")

tr = pytextrank.TextRank()
nlp.add_pipe(tr.PipelineComponent, name="textrank", last=True)


def listToString(s):
    str1 = ""
    for ele in s:
        str1 += ele

    return str1


def run_nlp(doc):
    doc = nlp(doc)
    with open('data//nlp_out//output.txt', 'w') as out_file:
        for p in doc._.phrases:
           print("{:.4f} {:5d}  {}".format(p.rank, p.count, p.text))
           out_file.write("{:.4f} {:5d}  {}".format(p.rank, p.count, p.text))


df_1 = pd.read_csv('data//nlp//*.csv', encoding='windows-1252')
doc1 = df_1.brief_summary.T
docs1 = listToString(doc1)

run_nlp(docs1)


```



```{python NLP Processor Stage II}

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import euclidean_distances


corpus = ['Compatibility', 'of', 'systems', 'of', 'linear', 'constraints', 'over', 'the', 'set', 'of', 'natural numbers','Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of', 'compatibilitz', 'construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types' ,'systems', 'and', 'systems of mixed types.']

def run_nlp2(corpus):
    vectorizer = CountVectorizer()
    features = vectorizer.fit_transform(corpus).todense()
    with open('data//nlp_out//stage2_output.txt', 'w') as out_file:
        for f in features:
             print(euclidean_distances(features[0],f) )
             out_file.write(euclidean_distances(features[0],f)

run_nlp2(corpus)


```



```{python}

import pytextrank
import spacy
from math import sqrt
import pandas as pd
import glob

path = "data//nlp_out/*.csv"

nlp = spacy.load("en_core_web_sm")

tr = pytextrank.TextRank()
nlp.add_pipe(tr.PipelineComponent, name="textrank", last=True)


def listToString(s):
    str1 = ""
    for ele in s:
        str1 += ele

    return str1


def run_nlp(doc,fname):
    doc = nlp(doc)
    with open('data//nlp_out//output' + fname + '.txt', 'w') as out_file:
        for p in doc._.phrases:
           print("{:.4f} {:5d}  {}".format(p.rank, p.count, p.text))
           out_file.write("{:.4f} {:5d}  {}".format(p.rank, p.count, p.text))


for fname in glob.iglob(path):
    df_1 = pd.read_csv(fname, encoding='windows-1252')
    doc1 = df_1.brief_summary.T
    docs1 = listToString(doc1)
    run_nlp(docs1)


```








