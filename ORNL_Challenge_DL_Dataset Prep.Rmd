---
title: "ORNL Data Challenge - Build Deep Learning Dataset"
author: "Jerome Dixon"
date: "7/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```




```{r}

library(reticulate)
use_condaenv("ORNL")

```




```{r}

library(here)
library(readxl)
library(httr)
library(jsonlite)
library(dplyr)
library(magrittr)
library(aws.s3)
library(tidyverse)
library(kableExtra)
library(purrr)
library(data.table)
library(tm)
library(caret)
library(keras)

```




```{r AWS ORNL Data Files}

aws_bucket <- "s3://ornl-data-challenge/"

bucket_df <- get_bucket_df(aws_bucket, check_region = F)

bucket_df$Key

#Download AWS files to your local directory

```




```{r Local ORNL Data Files}

#Sample Participants
participants_marked <- read_excel(here("data","participants_data", "Dataset2_Sample_Participant_Data_With_Syn_Columns_Marked.xlsx"))

#Eligibility Criteria
files_e <- list.files(path = here("data","eligibility_criteria"), pattern = "*.xlsx", full.names = T)

eligibility_rules <- sapply(files_e, read_excel, simplify=FALSE) %>% 
bind_rows(.id = "id")

#Clinician Matches
files_c <- list.files(path = here("data","clinician_matches"), pattern = "*.xlsx", full.names = T)

clinician_matches <- sapply(files_c, read_excel, simplify=FALSE) %>% 
bind_rows(.id = "id")


```




```{r Test Files}

test_data_titles <- clinician_matches[2,c(3:51)] %>% t()
test_data_titles %<>% as.data.frame() %>% unique()
rownames(test_data_titles) <- c()
test_data_titles = test_data_titles[-1,]
test_data_titles %<>% as.list()

test_data_nct <- clinician_matches[1,c(3:51)] %>% t()
test_data_nct %<>% as.data.frame() %>% unique()
rownames(test_data_nct) <- c()
test_data_nct = test_data_nct[-1,]
test_data_nct %<>% as.list()


write.csv(test_data_nct, "data/test_trial_ids.csv")

```




```{r Test Files API Data}

### This does not work. Test data appears to be made up IDs. Will subset and create our own data from National Cancer Institute (NCI) Cancer Clinical Trials API..

trial_ids <- as.data.frame(test_data_nct) %>% t()

row.names(trial_ids) <- c(1:length(trial_ids))
trial_ids$trial_id <- trial_ids[,1]


base_url <- "https://clinicaltrialsapi.cancer.gov/v1/clinical-trial/"

trial_ids$URL <- paste0(base_url,trial_ids$trial_id, sep="")

URL <- as.data.frame(trial_ids$URL)
names(URL) <- "URL"

test_data_targets <- map(trial_ids$URL, ~{

  Sys.sleep(sample(seq(0.5, 2.5, 0.5), 1)) 

  res <- httr::GET(.x)

  httr::content(res, as="text", encoding="UTF-8") %>% 
    fromJSON(flatten=TRUE) 

})


```



```{r Clinical Trials Data From NCI API}

#Selecting 15 (?size=15) Clinical Trials per cancer site. We will train with 12 clinical trials per site. Use 3 clinical trials per site for testing.
base_url <- "https://clinicaltrialsapi.cancer.gov/v1/clinical-trials?size=15&include=official_title&include=brief_summary&anatomic_sites"

cancer_sites <- read_csv("data/cancer-sites.csv")

cancer_sites$URL <- paste0(base_url,'=',cancer_sites$`Cancer Sites`,sep="")

URL <- as.data.frame(cancer_sites$URL)
names(URL) <- "URL"

clinical_trials <- map(URL$URL, ~{

  Sys.sleep(sample(seq(0.5, 2.5, 0.5), 1)) 

  res <- httr::GET(.x)

  httr::content(res, as="text", encoding="UTF-8") %>% 
    fromJSON(flatten=TRUE) %>% 
    as_tibble() 

})


```




```{r Create Train and Test Data Sets}


names(clinical_trials) <- c(1:25)

dat <- data.frame()

for(i in seq_along(clinical_trials)) {
   title <- clinical_trials[[i]][["trials"]] 
   site <- clinical_trials[[i]][["total"]]

   dom_nlp <- cbind(title, site) %>%
      unnest(cols=c('official_title','brief_summary'))

   dat <- rbind(dat, dom_nlp)
   
}

dat$site <- as.factor(dat$site)

names(dat) <- c('official_title','brief_summary','total')


set.seed(1997)

trainIndex <- createDataPartition(dat$total, p = .85, list = FALSE, times = 1)

clinical_trials_train <- dat[ trainIndex,]
clinical_trials_test  <- dat[-trainIndex,]

clinical_trials_train %<>% group_by(total)
test_df <-  split(clinical_trials_train, clinical_trials_train$total)

clncl_train <- as.list(x=test_df)
names(clncl_train) <- c(1:25)


```




```{r Create Files for NLP Processing}


for(i in seq_along(clncl_train)) {
   title <- clncl_train[[i]][["official_title"]] 
   summary <- clncl_train[[i]][["brief_summary"]] 
   target_site <- clncl_train[[i]][["total"]]

   dom_nlp <- cbind(title, summary, target_site)

   write.csv(dom_nlp, paste0("data/nlp_stage1/", names(clncl_train)[i],".csv"))
}


```



```{python Pytextrank NLP Processing Functions }
import pytextrank
import spacy
from math import sqrt
import pandas as pd
import glob
import csv

path =  "data//nlp_stage1/*.csv"
path2 = "data//nlp_stage3/*.csv"

nlp = spacy.load("en_core_web_sm")
nlp.max_length = 30030030 

tr = pytextrank.TextRank()
nlp.add_pipe(tr.PipelineComponent, name="textrank", last=True)


def listToString(s):
    str1 = ""
    for ele in s:
        str1 += ele

    return str1


def run_nlp_text_summary(path):
    text_summary_metrics = []
    for fname in glob.iglob(path):
        df_1 = pd.read_csv(fname, encoding='windows-1252')
        doc1 = df_1.summary.T
        df_id = df_1.index + 1
```


```{python Pytextrank NLP Processing Functions }
target_site = df_1.target_site
        row_id = target_site + df_id
        docs1 = listToString(doc1)
        doc = nlp(docs1, target_site)
        for p in doc._.phrases:
            #print("{:.4f} {:5d} {} {} ".format(p.rank, p.count, p.text, target_site))
            text_summary_metrics.append("{} {:.4f} {:5d} {} {} ".format(row_id, p.rank, p.count, p.text, target_site))
    
    return text_summary_metrics
    
    

def run_nlp_trials_title(path):
    trial_title_metrics = []
    for fname in glob.iglob(path):
        df_1 = pd.read_csv(fname, encoding='windows-1252')
        doc1 = df_1.title.T
        df_id = df_1.index + 1
        target_site = df_1.target_site
        row_id = target_site + df_id
        docs1 = listToString(doc1)
        doc = nlp(docs1, target_site)
        for p in doc._.phrases:
            #print("{} {:.4f} {:5d} {} {} ".format(p.rank, p.count, p.text, target_site))
            trial_title_metrics.append("{} {:.4f} {:5d} {} {} ".format(row_id, p.rank, p.count, p.text, target_site))
    
    return trial_title_metrics
    
    
    
def run_nlp_merged_keywords(path2):
    merged_keywords = []
    for fname in glob.iglob(path2):
        df_1 = pd.read_csv(fname, encoding='windows-1252')
        doc1 = df_1.merged_keywords_all.T
        target_site = df_1.target_site
        row_id = df_1.row_id
        docs1 = listToString(doc1)
        doc = nlp(docs1, target)
        for p in doc._.phrases:
            #print("{:.4f} {:5d} {} {} ".format(p.rank, p.count, p.text, target_site))
            merged_keywords.append("{} {:.4f} {:5d} {} ".format(row_id, p.rank, p.count, target_site))
    
    return merged_keywords
    
    
#To call python nlp functions:
#py$run_nlp_text_summary(py$path)
#py$run_nlp_trials_title(py$path)
#py$run_nlp_merged_keywords(py$path2)
    

```





```{r Clinical Trials Text Summary Metrics Dataframe}


nlp_summary_stage1 <- as.list(py$run_nlp_text_summary(py$path)) %>% 
  str_split(., " ")

nlp_summary_stage2 <- lapply(nlp_summary_stage1, function(z){ z[!is.na(z) & z != ""]}) 

nlp_summary_stage3 <- t(sapply(nlp_summary_stage2, `length<-`, max(lengths(nlp_summary_stage2))))

nlp_summary_stage4 <- as.data.frame(nlp_summary_stage3)

nlp_summary_stage4$merged_keywords <- paste(nlp_summary_stage4$V3,nlp_summary_stage4$V4,nlp_summary_stage4$V5,nlp_summary_stage4$V6,nlp_summary_stage4$V7,nlp_summary_stage4$V8)

nlp_summary_stage4 %<>% mutate(target_site = str_split(nlp_summary_stage4$V23, "\\s", simplify = TRUE)[, 1])

nlp_summary_stage5 <- nlp_summary_stage4 %>%  select(V1, V2, merged_keywords, target_site)

names(nlp_summary_stage5) <- c("similiarity_index", "nodes", "merged_keywords", "target_site")

nlp_summary_stage5$merged_keywords <- removeNumbers(nlp_summary_stage5$merged_keywords) 



```


```{r Clinical Trials Title NLP Metrics}


nlp_title_stage1 <- as.list(py$run_nlp_trials_title(py$path)) %>% 
  str_split(., " ")


nlp_title_stage2 <- lapply(nlp_title_stage1, function(z){ z[!is.na(z) & z != ""]})

nlp_title_stage3 <- t(sapply(nlp_title_stage2, `length<-`, max(lengths(nlp_title_stage2))))

nlp_title_stage4 <- as.data.frame(nlp_title_stage3)

nlp_title_stage4$merged_keywords <- paste(nlp_title_stage4$V3,nlp_title_stage4$V4,nlp_title_stage4$V5,nlp_title_stage4$V6,nlp_title_stage4$V7,nlp_title_stage4$V8)

nlp_title_stage4 %<>% mutate(target_site = str_split(nlp_title_stage4$V29, "\\s", simplify = TRUE)[, 1])

nlp_title_stage5 <- nlp_title_stage4 %>%  select(V1, V2, merged_keywords, target_site)

names(nlp_title_stage5) <- c("similiarity_index", "nodes", "merged_keywords", "target_site")

nlp_title_stage5$merged_keywords <- removeNumbers(nlp_title_stage5$merged_keywords) 



```





```{r Untidy Dataset}


DL_df <- inner_join(nlp_summary_stage5, nlp_title_stage5, by= 'target_site')
names(DL_df) <- c("similiarity_index.summary", "nodes.summary", "merged_keywords.summary", "target_site", "similiarity_index.title", "nodes.title", "merged_keywords.title")

DL_df$merged_keywords_all <- paste(DL_df$merged_keywords.summary, DL_df$merged_keywords.title) %>% 
  gsub("[\r\n]", "",.)


write.csv(DL_df, "data/nlp_stage2/Deep_Learning_Dataset.csv")


```




```{r Merge Title and Summary for NLP Analysis}


nlp_merge_stage1 <- as.list(py$run_nlp_merged_keywords(py$path2)) %>% 
  str_split(., " ")


nlp_merge_stage2 <- lapply(nlp_merged_stage1, function(z){ z[!is.na(z) & z != ""]})

nlp_merge_stage3 <- t(sapply(nlp_merged_stage2, `length<-`, max(lengths(nlp_merged_metrics2))))

nlp_merge_stage4 <- as.data.frame(nlp_merged_stage3)


nlp_merge_stage5 <- nlp_merged_stage4 %>% mutate(target_site = str_split(nlp_merge_stage4$V4, "\\s", simplify = TRUE)[, 1])

nlp_merge_stage5 %<>% select(V1, V2, target_site)

names(nlp_merge_stage5) <- c("merged_similiarity_index", "merged_nodes", "target_site")

write.csv(nlp_merge_stage5, paste0("data/nlp_stage3/nlp_merge_stage5.csv"))



```




```{python Final Tidy Dataframe}

import pytextrank
import spacy
from math import sqrt
import pandas as pd
import glob
import csv

path =  "data//nlp_stage1/*.csv"
path2 = "data//nlp_stage3/*.csv"

nlp = spacy.load("en_core_web_sm")
nlp.max_length = 30030030 

tr = pytextrank.TextRank()
nlp.add_pipe(tr.PipelineComponent, name="textrank", last=True)


def listToString(s):
    str1 = ""
    for ele in s:
        str1 += ele

    return str1
    
    
def run_nlp_text_summary_no_words(path):
    text_summary_metrics = []
    for fname in glob.iglob(path):
        df_1 = pd.read_csv(fname, encoding='windows-1252')
        doc1 = df_1.brief_summary.T
        target_site = df_1.target_site
        docs1 = listToString(doc1)
        doc = nlp(docs1, site)
        for p in doc._.phrases:
            #print("{:.4f} {:5d} {} {} ".format(p.rank, p.count, site))
            text_summary_metrics.append("{:.4f} {:5d} {} ".format(p.rank, p.count, target_site))
    
    return text_summary_metrics
    
    
def run_nlp_trials_title_no_words(path):
    trial_title_metrics = []
    for fname in glob.iglob(path):
        df_1 = pd.read_csv(fname, encoding='windows-1252')
        doc1 = df_1.official_title.T
        target_site = df_1.target_site
        docs1 = listToString(doc1)
        doc = nlp(docs1, site)
        for p in doc._.phrases:
            #print("{:.4f} {:5d} {} {} ".format(p.rank, p.count, site))
            trial_title_metrics.append("{:.4f} {:5d} {} {} ".format(p.rank, p.count, target_site))
    
    return trial_title_metrics
    
    

```



```{r}

#Summary NLP metrics
nlp_summary_stage1_tidy <- as.list(py$run_nlp_text_summary_no_words(py$path)) %>% 
  str_split(., " ")
nlp_summary_stage2_tidy <- lapply(nlp_summary_stage1_tidy, function(z){ z[!is.na(z) & z != ""]}) 
nlp_summary_stage3_tidy <- t(sapply(nlp_summary_stage2_tidy, `length<-`, max(lengths(nlp_summary_stage2_tidy))))
nlp_summary_stage4_tidy <- as.data.frame(nlp_summary_stage3_tidy)
nlp_summary_stage5_tidy <- nlp_summary_stage4_tidy %>%  select(V1, V2, target_site)

names(nlp_summary_stage5_tidy) <- c("similiarity_index", "nodes", "target_site")



#Title NLP metrics
nlp_title_stage1_tidy <- as.list(py$run_nlp_trials_title_no_words(py$path)) %>% 
  str_split(., " ")
nlp_title_stage2_tidy <- lapply(nlp_title_stage1_tidy, function(z){ z[!is.na(z) & z != ""]})
nlp_title_stage3_tidy <- t(sapply(nlp_title_stage2_tidy, `length<-`, max(lengths(nlp_title_stage2_tidy))))
nlp_title4 <- as.data.frame(nlp_title_stage3_tidy)
nlp_title_stage5_tidy <- nlp_title_stage4tidy %>%  select(V1, V2, target_site)

names(nlp_title_stage5_tidy) <- c("similiarity_index", "nodes", "target_site")

#Join Title and Summary datasets
DL_df_tidy_stage1 <- inner_join(nlp_summary_stage5_tidy, nlp_title_stage5_tidy, by= 'target_site')
names(DL_df_tidy) <- c("similiarity_index.summary", "nodes.summary", "target_site", "similiarity_index.title", "nodes.title", "title")

#Join Title, Summary, and Merged datasets
DL_df_tidy_stage2 <- inner_join(DL_df_tidy_stage1, nlp_merge_stage5, by= c('target_site'))

# Filter based on Pareto Grouping
DL_df_tidy_stage2$similiarity_index.summary <- as.numeric(DL_df_tidy_stage2$similiarity_index.summary)
DL_df_tidy_stage2$similiarity_index.title <- as.numeric(DL_df$similiarity_index.title)
DL_df_tidy_stage2$pareto <- ntile(DL_df_stage2$similiarity_index.summary, 20)  

final_dl_df <- DL_df_tidy2 %>% filter(pareto == 20 & similiarity_index.summary > .15)

write.csv(final_dl_df, paste0("data/nlp_stage4/clinical_trials_nlp.csv"))


```

